{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xo..\n",
      "....\n",
      ".¤..\n",
      ".@..\n",
      "\n",
      "Correspondance ID/position:\n",
      "3 7 11 15 \n",
      "2 6 10 14 \n",
      "1 5 9 13 \n",
      "0 4 8 12 \n",
      "\n",
      "Position du joueur (x) : (3, 0)\n"
     ]
    }
   ],
   "source": [
    "from environment import Game\n",
    "import numpy as np\n",
    "\n",
    "g = Game(4, 4)\n",
    "q = np.zeros((16, 4))\n",
    "print(g)\n",
    "print(\"Correspondance ID/position:\")\n",
    "print(g.affiche(debug=True))\n",
    "print(\"Position du joueur (x) :\", g.position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainement\n",
    "\n",
    "En utilisant le Q-learning, on va entrainer un agent à jouer.\n",
    "\n",
    "Je défini `g.wrong_action_p` à sa valeur par défaut. Cette variable correspond aux situations aléatoire pouvant survenir dans la vie, traduit dans ce jeu comme un pourcentage de change de faire un mouvement différent de celui demandé.\n",
    "\n",
    "`alpha` est le taux d'apprentissage, `gamma` est le facteur de réduction de la récompense future.\n",
    "\n",
    "On réitère une partie sur le même plateau `iterations` fois.\n",
    "\n",
    "En début de renforcement, on privilégie l'exploration, puis on exploite ce que l'on connait de plus en plus, au fil des parties. \n",
    "Ceci est traduit par la variable `epsilon` (tableau de flottant aléatoire) qui est ajouté à la valeur de Q pour la position actuelle.\n",
    "\n",
    "J'utilise `np.random.randn` qui génère des nombres aléatoires suivant une loi normale gaussienne centrée réduite.\n",
    "Ou comme le dit si bien la documentation de numpy :\n",
    "> with random floats sampled from a univariate “normal” (Gaussian) distribution of mean 0 and variance 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.wrong_action_p = 0.1\n",
    "iterations = 1000\n",
    "alpha = 0.1 # taux d’apprentissage\n",
    "gamma = 0.9 # interet des récompenses futurs\n",
    "for _ in range(iterations):\n",
    "    g.reset()\n",
    "    is_final = False\n",
    "    while not is_final:\n",
    "        state_id = g._get_state()\n",
    "        # Au début du renforcement, on explore aléatoirement\n",
    "        # plus on avance, plus on exploite ce que l'on a appris\n",
    "        epsilon = np.random.randn(1, 4) * (1. / (iterations + 1))\n",
    "        mov = np.argmax(q[state_id, :] + epsilon)\n",
    "        (new_state_id, reward, is_final, _) = g.move(mov)\n",
    "        q[state_id, mov] += alpha * (reward + gamma * np.max(q[new_state_id, :]) - q[state_id, mov])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Démonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  7\n",
      "xo..  .o..  .o..  .o..  .o..\n",
      "....  x...  ....  ....  ....\n",
      ".¤..  .¤..  x¤..  .¤..  .¤..\n",
      ".@..  .@..  .@..  x@..  .x..\n",
      "        \n",
      "[[-0.109      -0.1        -0.1         9.68080202]\n",
      " [-0.199      -0.199       6.88233248 -0.199     ]\n",
      " [-0.58935714 -0.58519851  4.94730056 -0.59855283]\n",
      " [-2.18128769 -1.82093062  2.39584532 -1.9       ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-1.          1.10715071 -0.77255306 -0.80190512]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.1         2.0821353   0.         -0.1       ]\n",
      " [-0.31522042 -0.29701    -0.385219   -0.25632527]\n",
      " [-0.55485657 -0.5516245  -0.46619566 -0.458848  ]\n",
      " [-1.09       -1.         -0.51789433 -0.5550534 ]\n",
      " [-0.199      -0.0466842  -0.1        -0.199     ]\n",
      " [-0.31311361 -0.29701    -0.361      -0.3058309 ]\n",
      " [-0.46896652 -0.41511377 -0.47334781 -0.3940399 ]\n",
      " [-0.4900995  -0.4261888  -0.4988919  -0.4900995 ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Show a game\n",
    "g.wrong_action_p = 0.0\n",
    "g.reset()\n",
    "is_final = False\n",
    "score = 0\n",
    "s = g.__str__()\n",
    "while not is_final:\n",
    "    state_id = g._get_state()\n",
    "    mov = np.argmax(q[state_id, :])\n",
    "    (new_state_id, reward, is_final, _) = g.move(mov)\n",
    "    score += reward\n",
    "    s = '\\n'.join([x + '  ' + y for x, y in zip(s.split('\\n'), g.__str__().split('\\n'))])\n",
    "print(\"Score: \", score)\n",
    "print(s)\n",
    "print(q)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chemin optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "↓ ↓ ↓ → "
     ]
    }
   ],
   "source": [
    "current = g.start\n",
    "arrows = [\"↑\", \"←\", \"↓\" , \"→\"]\n",
    "while current != g.end:\n",
    "    current_id = g._position_to_id(*current)\n",
    "    mov = np.argmax(q[current_id, :])\n",
    "    print(arrows[mov], end=' ')\n",
    "    current = (current[0] + g.MOVEMENTS[mov][0], current[1] + g.MOVEMENTS[mov][1])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
